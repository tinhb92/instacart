---
title: "R Notebook"
output: html_notebook
---

```{r}
library(data.table)
library(dplyr)
library(tidyr)
library(feather)
library(ggplot2)

# Load Data ---------------------------------------------------------------
path <- "/media/tin/DATA/Kaggle/Instacart/Input/"

rf <- function(x) {
  read_feather(file.path(path, x))
}

orderp <- rf("order_products__prior.feather")
ordert <- rf("order_products__train.feather")
orders <- rf("orders.feather")

orders$eval_set <- as.factor(orders$eval_set)
```

```{r}
# step 2
ordert$user_id <- orders$user_id[match(ordert$order_id, orders$order_id)]
orderp <- orders %>% inner_join(orderp, by = "order_id")
```

```{r}
# step 3
prd <- orderp %>%
  arrange(user_id, order_number, product_id) %>%
  group_by(user_id, product_id) %>%
  mutate(product_time = row_number()) %>% #ranking, counter for times purchased
  ungroup() %>%
  group_by(product_id) %>% #by products, aggragate over all orders and all customers
  summarise(
    prod_orders = n(),
    prod_reorders = sum(reordered),
    prod_first_orders = sum(product_time == 1),
    prod_second_orders = sum(product_time == 2)
  )
```

```{r}
prd <- prd %>%
  mutate(prod_reorder_probability = prod_second_orders / prod_first_orders) %>%
  mutate(prod_reorder_times = 1 + prod_reorders / prod_first_orders) %>%
  mutate(prod_reorder_ratio = prod_reorders / prod_orders) %>%
  select(-prod_reorders, -prod_first_orders, -prod_second_orders)

gc()
```

```{r}
# Users -------------------------------------------------------------------
users <- orders %>%
  filter(eval_set == "prior") %>%
  group_by(user_id) %>%
  summarise(
    user_orders = max(order_number),
    user_period = sum(days_since_prior_order, na.rm = T),
    user_mean_days_since_prior = mean(days_since_prior_order, na.rm = T)
  )

us <- orderp %>%
  group_by(user_id) %>%
  summarise(
    user_total_products = n(),
    user_reorder_ratio = sum(reordered == 1) / sum(order_number > 1),
    user_distinct_products = n_distinct(product_id)
  )

users <- users %>% 
  inner_join(us) %>%
  mutate(user_average_basket = user_total_products / user_orders)

us <- orders %>%
  filter(eval_set != "prior") %>%
  select(user_id, order_id, eval_set,
         time_since_last_order = days_since_prior_order,
         order_number, order_dow, order_hour_of_day)  #additional details for train and test sets

users <- users %>% inner_join(us)
# combine prior and train & test infomation for each user

rm(us)
gc()
```
```{r}
# Database ----------------------------------------------------------------
# righttttttt, this lists down all previously purchased products of a user
# candidate for train and set later

data <- orderp %>%
  group_by(user_id, product_id) %>% 
  summarise(
    up_orders = n(),
    up_first_order = min(order_number),
    up_last_order = max(order_number),
    up_average_cart_position = mean(add_to_cart_order), #analyse the sequence of purchased product
    ave_dow = mean(order_dow),
    ave_hour = mean(order_hour_of_day),
    ave_freq = mean(days_since_prior_order)) #additional features

# rm(orderp, orders)


```

```{r}
data <- data %>% 
  inner_join(prd, by = "product_id") %>%
  inner_join(users, by = "user_id") %>%
  mutate(up_order_rate <- up_orders / user_orders) %>%
  mutate(up_orders_since_last_order <- user_orders - up_last_order) %>%
  mutate(up_order_rate_since_first_order <- up_orders / (user_orders - up_first_order + 1)) %>%
  left_join(ordert %>% select(user_id, product_id, reordered), 
            by = c("user_id", "product_id"))

rm(ordert, prd, users)
gc()
```

```{r}
# Step 5: Train / Test datasets ---------------------------------------------------
train <- as.data.frame(data[data$eval_set == "train",]) %>%
  select(-eval_set, -user_id, -product_id, -order_id)

train$reordered[is.na(train$reordered)] <- 0

test <- as.data.frame(data[data$eval_set == "test",]) %>%
  select(-eval_set, -user_id, -reordered)

rm(data)
gc()
```

```{r}
browseVignettes(package = "Rcpp")
```

```{r}
library(xgboost)

params <- list(
  "objective"           = "reg:logistic",
  "eval_metric"         = "logloss",
  "eta"                 = 0.1,
  "max_depth"           = 6,
  "min_child_weight"    = 10,
  "gamma"               = 0.70,
  "subsample"           = 0.76,
  "colsample_bytree"    = 0.95,
  "alpha"               = 2e-05,
  "lambda"              = 10
)

subtrain <- train #%>% sample_frac(0.1)
X <- xgb.DMatrix(as.matrix(subtrain %>% select(-reordered)), label = subtrain$reordered)

# X <- xgb.DMatrix(as.matrix(train %>% select(-reordered)), label = train$reordered)

model <- xgboost(data = X, params = params, nrounds = 80)

# importance <- xgb.importance(colnames(X), model = model)
# xgb.ggplot.importance(importance)
# 
# rm(X, importance, subtrain)
gc()
```

```{r}
# Apply model -------------------------------------------------------------

#test data
X <- xgb.DMatrix(as.matrix(test %>% select(-order_id, -product_id)))

test$reordered <- predict(model, X)
test$reordered <- (test$reordered > 0.21) * 1 #* 1 to convert True False to 0 1
#originally 0.21

#for orders with atleast 1 product
submission <- test %>%
  filter(reordered == 1) %>%
  group_by(order_id) %>%
  summarise(
    products = paste(product_id, collapse = " ")
  )

missing <- data.frame(
  order_id = unique(test$order_id[!test$order_id %in% submission$order_id]),
  products = "None"
)

submission <- submission %>% bind_rows(missing) %>% arrange(order_id)
```

```{r}
path <- "/media/tin/DATA/Kaggle/Instacart/Output/" 
write.csv(submission, file = file.path(path,"submit_25 features.csv"), row.names = F)
```




Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
